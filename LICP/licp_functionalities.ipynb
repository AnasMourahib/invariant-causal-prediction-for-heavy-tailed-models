{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7005bc0c-3207-4799-b1cc-620d43af1c1d",
   "metadata": {},
   "source": [
    "This notebook shows the functionalities of several LICP related methods. We use two different approaches\n",
    "1. loli\n",
    "2. pirca\n",
    "\n",
    "Loli is the code relating to the published paper https://proceedings.mlr.press/v244/mey24a.html .\n",
    "pirca is the code relating to an unpublished paper and has several relaxations over loli, targeting the root cause identification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "256f5875-c41c-4e54-b186-aabe8e3ca1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5406270879677607\n"
     ]
    }
   ],
   "source": [
    "n=100\n",
    "import numpy as np\n",
    "A=np.random.pareto(3,size=(n,1))\n",
    "B=np.random.pareto(3,size=(n,1))\n",
    "print(min([np.var(A),np.var(B)])/max([np.var(A),np.var(B)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b1861-4a56-4cfb-9b67-9cfe82f94d33",
   "metadata": {},
   "source": [
    "## We start with loli: loli makes a normal noise assumption, hass little robustness against deviations from that, and has no robustness against the homogeneity assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "720a7f81-af00-447d-a520-65af78875c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import loli\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c46eb7-03c0-4b5e-afed-48d874ee6916",
   "metadata": {},
   "source": [
    "### The first experiment shows that loli works under the gaussian assumptiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78292e6-d621-42ab-bf89-bbf2593a198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parents of variable Y  under normal noise: {0}\n"
     ]
    }
   ],
   "source": [
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "np.random.seed(42)\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "X_loli=[]\n",
    "Y_loli=[]\n",
    "\n",
    "# We have the simple causal structure X0->Y->X1, which is in practice assumed to be unknown\n",
    "# We first generate data where all variables act 'normally', and save this in a dictionary\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.normal(0,1,size=(n,1))\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "X_loli.append(np.concatenate([X_normal['0'],X_normal['1']],axis=1)) # Data from each environment is appended to a list\n",
    "Y_loli.append(X_normal['Y'])\n",
    "    \n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.normal(0,1,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "X_loli.append(np.concatenate([X_faulty['0'],X_faulty['1']],axis=1))\n",
    "Y_loli.append(X_faulty['Y'])\n",
    "\n",
    "\n",
    "B=1000\n",
    "plausibleS=loli.gauss(X_loli,Y_loli,B=B)\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "print('Estimated parents of variable Y  under normal noise:', supphat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da558b9-88f2-4ba2-8d69-6ab7a26c21c7",
   "metadata": {},
   "source": [
    "### If we change the target noise to a heavy tailed distribution loli fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ef9fbd-d91d-47bb-b897-d74a8ba5ad52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parents of variable Y  under heavy tailed noise: {}\n",
      "Note: depending on the random seed we might get different wrong answers, or the correct answer by chance\n"
     ]
    }
   ],
   "source": [
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "np.random.seed(41)\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "X_loli=[]\n",
    "Y_loli=[]\n",
    "\n",
    "# We have the simple causal structure X0->X1->X2, which is in practice assumed to be unknown\n",
    "# We first generate data where all variables act 'normally', and save this in a dictionary\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.pareto(3,size=(n,1)) # THIS IS NOT NORMAL HERE\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1)) \n",
    "X_loli.append(np.concatenate([X_normal['0'],X_normal['1']],axis=1)) # Data from each environment is appended to a list\n",
    "Y_loli.append(X_normal['Y'])\n",
    "    \n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.pareto(3,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1)) \n",
    "X_loli.append(np.concatenate([X_faulty['0'],X_faulty['1']],axis=1))\n",
    "Y_loli.append(X_faulty['Y'])\n",
    "\n",
    "\n",
    "B=1000\n",
    "plausibleS=loli.gauss(X_loli,Y_loli,B=B)\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "print('Estimated parents of variable Y  under heavy tailed noise:', supphat)\n",
    "print('Note: depending on the random seed we might get different wrong answers, or the correct answer by chance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0745a3f-6dec-40f2-a27d-ce7cf5ddf585",
   "metadata": {},
   "source": [
    "### If the homogeneous noise assumption is violated, loli fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e3b8ca-0b5f-4c10-a3a9-faf02adc1749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parents of variable Y  under homogeneity violation: {}\n"
     ]
    }
   ],
   "source": [
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "np.random.seed(42)\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "X_loli=[]\n",
    "Y_loli=[]\n",
    "\n",
    "# We have the simple causal structure X0->Y->X1, which is in practice assumed to be unknown\n",
    "# We first generate data where all variables act 'normally', and save this in a dictionary\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.normal(0,1,size=(n,1))  # TARGET NOISE IS NOT HOMOGENOUS\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "X_loli.append(np.concatenate([X_normal['0'],X_normal['1']],axis=1)) # Data from each environment is appended to a list\n",
    "Y_loli.append(X_normal['Y'])\n",
    "    \n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.normal(0,2,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "X_loli.append(np.concatenate([X_faulty['0'],X_faulty['1']],axis=1))\n",
    "Y_loli.append(X_faulty['Y'])\n",
    "\n",
    "\n",
    "B=1000\n",
    "plausibleS=loli.gauss(X_loli,Y_loli,B=B)\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "print('Estimated parents of variable Y  under homogeneity violation:', supphat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7069c6-b059-4d82-bfee-5aa196a2a8f8",
   "metadata": {},
   "source": [
    "## Now we test if the several relations of pirca can deal with the violations of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f39ab4-c9c5-428a-902a-357e88c0b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LICP.licp import pirca\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79eb16-9418-48a4-9bee-413d4b47d9a8",
   "metadata": {},
   "source": [
    "#### pirca has several relaxations over loli. The first relaxation is the removal of the gaussian noise assumption. Instead pirca uses a permutation test. So we expect that this setting has (some) robustness against heavy tails, but not against homegeneity violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d122c45-bd27-478f-86c6-b8f3a2acbe90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      4\u001b[0m X\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros((n,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mLinearRegression\u001b[49m()\n\u001b[0;32m      7\u001b[0m pirca_inst\u001b[38;5;241m=\u001b[39mpirca(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# We have the simple causal structure X0->Y->X1, which is in practice assumed to be unknown\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# We first generate data where all variables act 'normally', and save this in a dictionary\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "np.random.seed(42)\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "model=LinearRegression()\n",
    "pirca_inst=pirca(model=model)\n",
    "\n",
    "# We have the simple causal structure X0->Y->X1, which is in practice assumed to be unknown\n",
    "# We first generate data where all variables act 'normally', and save this in a dictionary\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.normal(0,1,size=(n,1))\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.normal(0,1,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "data=[X_normal,X_faulty]\n",
    "\n",
    "pirca_inst.compute_parents(data,targets=['Y'],test='permutation')\n",
    "\n",
    "B=1000\n",
    "plausibleS=[set(s) for s in pirca_inst.plausible['Y']]\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "print('Estimated parents of variable Y  under normal noise:', pirca_inst.parents)\n",
    "\n",
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "\n",
    "\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.pareto(3,size=(n,1)) # THIS IS NOT NORMAL HERE\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1)) \n",
    "\n",
    "    \n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.pareto(3,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1)) \n",
    "\n",
    "data=[X_normal,X_faulty]\n",
    "\n",
    "pirca_inst.compute_parents(data,targets=['Y'],test='permutation')\n",
    "\n",
    "B=1000\n",
    "plausibleS=[set(s) for s in pirca_inst.plausible['Y']]\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "\n",
    "print('Estimated parents of variable Y  under heavy tailed noise:', pirca_inst.parents)\n",
    "\n",
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "\n",
    "\n",
    "# We have the simple causal structure X0->Y->X1, which is in practice assumed to be unknown\n",
    "# We first generate data where all variables act 'normally', and save this in a dictionary\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.normal(0,1,size=(n,1))  # TARGET NOISE IS NOT HOMOGENOUS\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.normal(0,2,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "data=[X_normal,X_faulty]\n",
    "\n",
    "\n",
    "pirca_inst.compute_parents(data,targets=['0','Y','1'],test='permutation')\n",
    "\n",
    "B=1000\n",
    "plausibleS=[set(s) for s in pirca_inst.plausible['Y']]\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "\n",
    "print('Estimated parents of variable Y  under homogeneity violation:', pirca_inst.parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f9442-7d88-4153-99be-0d6b9fd7956b",
   "metadata": {},
   "source": [
    "### Next to the 'permutation' test, we have a different test best on confidence intervals, here indicated with 'bootstrap'. The main difference: all previous methods try to find plausible sets in an absolute sense (meaning plausible sets ensure noise homogeneity). The 'bootstrap' approach looks for 'the most' plausible sets, i.e. finding subsets that generate the most noise homogeneity. The main caveat: depending on the specific data distribution this can lead to wrong conlusions, let's see what happens in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e037eee-368c-453a-8c28-59de46b3a700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Phase\n",
      "|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%\n",
      " Computing Plausible Sets\n",
      "|█████████████████████████---------------------------------------------------------------------------| 25.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amey\\OneDrive - ASML\\Documents\\Github\\Code_for_LICP\\LICP\\licp.py:135: UserWarning: No timestamps provided. Calculating timestamps based on the assumptions that start and end time of all measurements are the same.\n",
      "  warnings.warn('No timestamps provided. Calculating timestamps based on the assumptions that start and end time of all measurements are the same.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parents of variable Y  under normal noise: {'Y': {('0',): 0.99110422102479}}███████████████| 100.00%\n",
      "Model Training Phase\n",
      "|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 180.00%\n",
      " Computing Plausible Sets\n",
      "|██████████████████████████████████████████████████--------------------------------------------------| 50.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amey\\OneDrive - ASML\\Documents\\Github\\Code_for_LICP\\LICP\\licp.py:135: UserWarning: No timestamps provided. Calculating timestamps based on the assumptions that start and end time of all measurements are the same.\n",
      "  warnings.warn('No timestamps provided. Calculating timestamps based on the assumptions that start and end time of all measurements are the same.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parents of variable Y  under heavy tailed noise: {'Y': {('0',): 0.7627996227081058, ('1',): 0.5293688956639675}}\n",
      "Model Training Phase\n",
      "|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 126.67%\n",
      " Computing Plausible Sets\n",
      "|████████--------------------------------------------------------------------------------------------| 8.33%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amey\\OneDrive - ASML\\Documents\\Github\\Code_for_LICP\\LICP\\licp.py:135: UserWarning: No timestamps provided. Calculating timestamps based on the assumptions that start and end time of all measurements are the same.\n",
      "  warnings.warn('No timestamps provided. Calculating timestamps based on the assumptions that start and end time of all measurements are the same.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parents of variable Y  under homogeneity violation: {'Y': {('0',): 0.2167594844867301, ('1',): 0.5755708956009704}, '0': {('1',): 0.1842458028027876}, '1': {('Y',): 0.8766156697221484}}\n"
     ]
    }
   ],
   "source": [
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "np.random.seed(42)\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "model=LinearRegression()\n",
    "pirca_inst=pirca(model=model)\n",
    "\n",
    "# We have the simple causal structure X0->Y->X1, which is in practice assumed to be unknown\n",
    "# We first generate data where all variables act 'normally', and save this in a dictionary\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.normal(0,1,size=(n,1))\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.normal(0,1,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "data=[X_normal,X_faulty]\n",
    "\n",
    "pirca_inst.compute_parents(data,targets=['Y'],test='bootstrap')\n",
    "\n",
    "B=1000\n",
    "plausibleS=[set(s) for s in pirca_inst.plausible['Y']]\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "print('Estimated parents of variable Y  under normal noise:', pirca_inst.parents)\n",
    "\n",
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "\n",
    "\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.pareto(3,size=(n,1)) # THIS IS NOT NORMAL HERE\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1)) \n",
    "\n",
    "    \n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.pareto(3,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1)) \n",
    "\n",
    "data=[X_normal,X_faulty]\n",
    "\n",
    "pirca_inst.compute_parents(data,targets=['Y'],test='bootstrap')\n",
    "\n",
    "B=1000\n",
    "plausibleS=[set(s) for s in pirca_inst.plausible['Y']]\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "\n",
    "print('Estimated parents of variable Y  under heavy tailed noise:', pirca_inst.parents)\n",
    "\n",
    "# We have a simple example with three variables for which we have each 1000 samples\n",
    "n=1000\n",
    "X=np.zeros((n,3))\n",
    "\n",
    "\n",
    "\n",
    "# We have the simple causal structure X0->Y->X1, which is in practice assumed to be unknown\n",
    "# We first generate data where all variables act 'normally', and save this in a dictionary\n",
    "X_normal={}\n",
    "X_normal['0']=np.random.normal(0,1,size=(n,1))\n",
    "X_normal['Y']=X_normal['0']+np.random.normal(0,1,size=(n,1))  # TARGET NOISE IS NOT HOMOGENOUS\n",
    "X_normal['1']=X_normal['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "# We then introduce a 'fault' into the system by increasing the variance of X1, which affects X1 and X2\n",
    "X_faulty={}\n",
    "X_faulty['0']=np.random.normal(0,4,size=(n,1))\n",
    "X_faulty['Y']=X_faulty['0']+np.random.normal(0,2,size=(n,1))\n",
    "X_faulty['1']=X_faulty['Y']+np.random.normal(0,2**(1/2),size=(n,1))\n",
    "\n",
    "data=[X_normal,X_faulty]\n",
    "\n",
    "\n",
    "pirca_inst.compute_parents(data,targets=['0','Y','1'],test='bootstrap')\n",
    "\n",
    "B=1000\n",
    "plausibleS=[set(s) for s in pirca_inst.plausible['Y']]\n",
    "if not not plausibleS:\n",
    "    supphat=set.intersection(*plausibleS)\n",
    "else:\n",
    "    supphat={}\n",
    "\n",
    "print('Estimated parents of variable Y  under homogeneity violation:', pirca_inst.parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ae7dd-1932-412d-bba3-53f7acbf1ec6",
   "metadata": {},
   "source": [
    "### Bootstrap does not work very well here, showing that we indeed need to adress the homogeneity condition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
